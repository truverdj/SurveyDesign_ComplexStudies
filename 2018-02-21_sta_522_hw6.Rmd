---
title: "STA 522 HW6 (Multi-Stage Sampling)"
author: "Daniel Truver"
date: "2/21/2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, dev = "pdf")
```

#### (1) Lohr, Chapter 6 Problem 4  

We begin with showing $\hat{t}_\psi$ is unbiased. 
$$
E(\hat{t}_\psi) 
= E\left( \frac{t_i}{\psi_i} \right) 
= \sum_{i} \left( \frac{t_i}{\psi_i} \right) \psi_i
= \sum_i t_i = t.
$$

```{r 1.lazyCalculation, include=FALSE}
psi = c(7/16, rep(3/16,3))
t = c(11,20,24,245)
variance = 0
for (i in 1:4){
  variance = variance + (t[i]/psi[i] - 300)^2 * psi[i]
}
```

The variacne of $\hat{t}_\psi$ is given by the following.
$$
\begin{aligned}
Var(\hat{t}_\psi) 
&= E\left[(\hat{t}_\psi - t)^2\right] \\
&= \sum_i \left( \frac{t_i}{\psi_i} - t \right)^2 \psi_i \\
&= (11(16/7) - 300)^2(7/16) + (20(16/3) - 300)^2(3/16)\\
&\quad\quad + (24(16/3) - 300)^2(3/16) + (245(16/3) - 300)^2(3/16) \\
&\approx 2.36 \times 10^5
\end{aligned}
$$

This variance is much worse than the variance of the $\pi$ps estimator used in section 6.1, an estimator which is also unbiased. Therefore, we do not have a reason to use this distribution for $\psi_i$. Intuitively, it does not make sense to put so much weight on the smallest of the supermarkets. 

#### (2) Lohr, Chapter 6 Problem 12

##### (a) Plotting Probability vs Number of Farms

```{r readThatStatepopData,echo=FALSE, message=FALSE, warning=FALSE}
library(dplyr)
library(ggplot2)
library(survey)
statepop = read.csv("statepop.csv")
M_0 = 255077536
statepop = statepop %>%
  mutate(psi = popn/M_0) %>%
  mutate(that = phys/psi) %>%
  mutate(countyunique = 1:nrow(.)) %>%
  mutate(wts = 1/psi)
n = nrow(statepop)
```


```{r, out.width="300px", out.height="300px", echo=FALSE}
ggplot(data = statepop, aes(x = psi, y = numfarm)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  xlab("psi") +
  ggtitle("Number of Farms in State vs Probability of Selection") +
  theme_bw()
```

There appears to be some positive correlation here, but it is not strong. Considering how much leverage Los Angeles county has, we cannot say in good conscience that this sampling design will be efficient for estimating total number of farms.

Does this make intuitive sense? Yes, the counties were sampled based on population size, but rural areas with plentiful farms tend to have lower population. 

##### (b) Total Number of Farms in US

The survey package seems to have a problem dealing in true with-replacement sampling. 

```{r totalFarmsInUS}
svy.farms = svydesign(ids = ~countyunique, probs = statepop$psi, data = statepop)
total.farms = svytotal(~numfarm, svy.farms)
```

```{r 2b.table, echo=FALSE}
knitr::kable(data.frame(total.farms),
             col.names = c("Total", "Standard Error"),
             row.names = FALSE, digits = 0, 
             caption = "Suspicious Estimate for Total Farms")
```

189 million farms in the U.S seems fairly suspicious.

If we use the Hansen-Hurwitz estimator for with-replacement sampling from Lohr (pg. 228), 
$$
\begin{aligned}
& \hat{t}_\psi = \frac{1}{n}\sum_{i=1}^n \frac{t_i}{\psi_i} \\
& \hat{Var}(\hat{t}_\psi) = \frac{1}{n}\frac{1}{n-1}\sum_{i=1}^n \left( \frac{t_i}{\psi_i} - \hat{t}_\psi \right)^2
\end{aligned}
$$
we get the following results:

```{r HansenEstimate}
hansen.hurwitz = function(totals, psi, n){
  estimate = 1/n * sum(totals/psi)
  variance = (1/n)*1/(n-1) * sum( (totals/psi - estimate)^2 )
  SE = sqrt(variance)
  return(list("estimate" = estimate, "SE" = SE))
}
total.farms = hansen.hurwitz(statepop$numfarm, statepop$psi, n)
```

```{r HansenTable, echo=FALSE}
knitr::kable(data.frame(total.farms), col.names = c("Total", "SE"),
             caption = "Less Suspicious Estimate of Total Farms")
```

A brief google search confirms the suspicion. Until I have a moment to ask about the survey package functionality in with-replacement surveys, I will use the function constructed above. 

\newpage
#### (3) Lohr, Chapter 6 Problem 13
##### (a) Plotting Veterans vs Probability 

```{r plot.3a, echo=FALSE, out.width="300px", out.height="300px"}
plot(statepop$psi, statepop$veterans, pch = "*",
     main = "Probability of Selecton vs Number of Veterans")
```

This correlation appears much stronger than the correlation between probability and number of farms. The $\pi$ps design will be efficient here.

Does this fit with intution? The counties are sampled proportional to population. Greater population, more veterans. 

##### (b) Total Number of Veterans 

```{r totalVets}
vets.total = hansen.hurwitz(statepop$veterans, statepop$psi, n)
```

```{r 3b.table, echo=FALSE}
knitr::kable(data.frame(vets.total), digits = 2, row.names = FALSE,
             col.names = c("Total", "SE"),
             caption = "Less Suspicious Estimate of Veterans in U.S.")
```

##### (c) Total Vietnam Veterans

```{r totalVietnamVets}
statepop = statepop %>%
  mutate(vietVets = veterans * (percviet/100))
vietVets.total = hansen.hurwitz(statepop$vietVets, statepop$psi, n)
```  

```{r 3c.table, echo=FALSE}
knitr::kable(data.frame(vietVets.total), col.names = c("Total", "SE"),
             caption = "Less Suspicious Estimate of Vietnam Veterans")
```

#### (4) National Crime Victimization Survey

```{r readNCVS}
ncvs = read.csv("ncvs2000.csv") %>%
  mutate(individual = 1:nrow(.)) %>%
  mutate(ppsu = 10*pstrat + ppsu)
svy.ncvs = svydesign(ids = ~ppsu,strata = ~pstrat, weights = ~pweight,data = ncvs)
```

##### (a) Total Crime in US

```{r 4a.totalCrime}
total.crime = svytotal(~numinc, svy.ncvs)
confi.crime = confint(total.crime) %>% data.frame()
confi.crime = cbind(total = total.crime[1], confi.crime)
```

```{r 4a.presResults, echo=FALSE}
knitr::kable(confi.crime, col.names = c("Total", "2.5%", "97.5%"),
             caption = "Total Crime Incidents Reported (2000)",
             row.names = FALSE)
```  

##### (b) Total Number of Crimes by Race

```{r crimesByRace}
ncvs = ncvs %>%
  mutate(white.inc = 1*(race == 1) * numinc) %>%
  mutate(black.inc = 1*(race == 2) * numinc) %>%
  mutate(nativ.inc = 1*(race == 3) * numinc) %>%
  mutate(asian.inc = 1*(race == 4) * numinc)
svy.ncvs = svydesign(ids = ~ppsu,strata = ~pstrat, weights = ~pweight,data = ncvs)
total.white.inc = svytotal(~white.inc, svy.ncvs)
confi.white.inc = confint(total.white.inc) %>% cbind(total.white.inc[1], .)
total.black.inc = svytotal(~black.inc, svy.ncvs)
confi.black.inc = confint(total.black.inc) %>% cbind(total.black.inc[1], .)
total.nativ.inc = svytotal(~nativ.inc, svy.ncvs)
confi.nativ.inc = confint(total.nativ.inc) %>% cbind(total.nativ.inc[1], .)
total.asian.inc = svytotal(~asian.inc, svy.ncvs)
confi.asian.inc = confint(total.asian.inc) %>% cbind(total.asian.inc[1], .)
```

```{r pres.4b, echo=FALSE}
res.4b = rbind(confi.white.inc,
               confi.black.inc,
               confi.nativ.inc,
               confi.asian.inc)
rownames(res.4b) = c("White", "Black", "Native American", "Asia-Pacific")
knitr::kable(res.4b, col.names = c("Total", "2.5%", "97.5%"),
             caption = "Total Crimes Reported by Race (2000)", row.names = TRUE)
```

##### (c) Medical Expenses of Crime

```{r medicalExpense}
total.expense = svytotal(~medexp, svy.ncvs)
confi.expense = confint(total.expense) %>% cbind(total.expense[1], .)
```

```{r pres.4c, echo=FALSE}
knitr::kable(confi.expense, col.names = c("Total", "2.5%", "97.5%"),
             caption = "Total Medical Expenses as a Result of Crime (2000)")
```

##### (d) Average Robberies per Person

```{r avgRobberies}
mean.rob = svymean(~robbery, svy.ncvs)
conf.rob = confint(mean.rob) %>% cbind(mean.rob[1], .)
```

```{r res.4d, echo=FALSE}
knitr::kable(conf.rob, col.names = c("Total", "2.5%", "97.5%"),
             caption = "Average Robberies reported per Person (2000)",
             row.names = FALSE, digits = 5)
```

#### (5) Return to NCVS with Incompetent Friend

First, we will explore what the results of our friend's method would be.

```{r friendMethod}
n = nrow(ncvs)
avg_friend = mean(ncvs$robbery)
SE_friend = sqrt(sum( (ncvs$robbery - avg_friend)^2 )/n)
noDesign = data.frame(avg_friend, SE_friend)
withDesign = data.frame(mean.rob) %>% unname()
```

```{r 5ComparisonTable, echo=FALSE}
knitr::kable(cbind(noDesign, withDesign), 
             col.names = c("Friend's AVG", "Friend's SE", "HT-AVG", "HT-SE"),
             row.names = FALSE)
```

Let $\tilde{y}$ denote our friend's estimate of the average robberies per person. 

We first note the the variance of $\tilde{y}$ is much higher than the variance of the Horvitz-Thompson Estimator. Immediately, this makes us suspicious. We want accuracy in our estimate, especially if we inted to craft policy or interventions based on this data. 

Also, this estimate is biased. 
$$
E(\tilde{y}) 
= E\left(\frac{1}{m}\sum_{i=1}^m y_i \right) 
= \frac{1}{m} \sum_{i = 1}^{M_0} y_i \psi_i
$$

A biased estimator with high variance isn't of much use to use when we have an unbiased estimator with lower variance.
